{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "848cb4b6-4a4b-4300-9299-64a53068d24f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CI/CD for Databricks Pipelines: Git Integration & Automated Deployment\n",
    "Demo Scenario:\n",
    "\n",
    "Objective: Show how to automate testing and deployment of a Delta Live Tables (DLT) pipeline using GitHub Actions and Databricks CLI.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Set up a Git repo with a DLT pipeline (e.g., the e-commerce ETL pipeline from Topic 1).\n",
    "\n",
    "Configure CI/CD:\n",
    "\n",
    "### Use databricks bundle to deploy pipelines as code.\n",
    "We have two of the branch: main and prd \n",
    "\n",
    "Add a GitHub Actions workflow to run unit tests (e.g., validate schema, data quality checks) on PR merges.\n",
    "\n",
    "Promote changes: Deploy from dev to prod using workspace-specific configs.\n",
    "\n",
    "Show & Tell Focus:\n",
    "\n",
    "Compare manual vs. automated deployments.\n",
    "\n",
    "Demo rollback on failure (e.g., broken schema change)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcbfd83a-3d68-4ec2-8ed5-3c42a132a373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "unit tests play a vital role for several reasons:\n",
    "\n",
    "Verification of Correctness: Unit tests verify the functionality of individual units of code, ensuring they behave as expected under various conditions.\n",
    "\n",
    "Early Bug Detection: By identifying bugs early in the development process, developers can address them promptly, reducing the probability of issues propagating to other parts of the system.\n",
    "\n",
    "Refactoring and Maintenance: Unit tests act as a safety net during code refactoring and maintenance, allowing developers to make changes confidently while ensuring consistent behavior.\n",
    "\n",
    "Regression Testing: Unit tests serve as regression tests, ensuring that new changes or features do not break existing functionality, thereby maintaining system stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5755ea4d-992f-4ff4-b9e3-8478a9fd81b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run Function (myfunction.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a3d3ec-0a06-443e-a2ba-1afa880e5077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "125a8b50-da26-4954-9bfe-ca798c97a77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "# Because this file is not a Databricks notebook, you\n",
    "# must create a Spark session. Databricks notebooks\n",
    "# create a Spark session for you by default.\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('integrity-tests') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Does the specified table exist in the specified database?\n",
    "def tableExists(tableName, dbName):\n",
    "  return spark.catalog.tableExists(f\"{dbName}.{tableName}\")\n",
    "\n",
    "# Does the specified column exist in the given DataFrame?\n",
    "def columnExists(dataFrame, columnName):\n",
    "  if columnName in dataFrame.columns:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "# How many rows are there for the specified value in the specified column\n",
    "# in the given DataFrame?\n",
    "def numRowsInColumnForValue(dataFrame, columnName, columnValue):\n",
    "  df = dataFrame.filter(col(columnName) == columnValue)\n",
    "\n",
    "  return df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33af96c3-4975-4456-b5e4-4eb54d128d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# envName     = \"env_dev\"\n",
    "tableName   = \"babynames\"\n",
    "dbName      = \"leodb\"\n",
    "columnName  = \"First Name\"\n",
    "columnValue = \"CHLOE\"\n",
    "\n",
    "if tableExists(tableName, dbName):\n",
    "\n",
    "  # df = spark.sql(f\"SELECT * FROM {envName}.{dbName}.{tableName}\")\n",
    "  df = spark.read.csv(f\"/Volumes/env_dev/leodb/babynames/babynames.csv\")\n",
    "\n",
    "  # And the specified column exists in that table...\n",
    "  if columnExists(df, columnName):\n",
    "    # Then report the number of rows for the specified value in that column.\n",
    "    numRows = numRowsInColumnForValue(df, columnName, columnValue)\n",
    "\n",
    "    print(f\"There are {numRows} rows in '{tableName}' where '{columnName}' equals '{columnValue}'.\")\n",
    "  else:\n",
    "    print(f\"Column '{columnName}' does not exist in table '{tableName}' in schema (database) '{dbName}'.\")\n",
    "else:\n",
    "  print(f\"Table '{tableName}' does not exist in schema (database) '{dbName}'.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f53310d-7f13-4c48-b275-e5d0605e3b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(f\"/Volumes/env_dev/leodb/babynames/babynames.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\")\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f64c1219-7810-4a52-a39d-bec47f67d4e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.read_csv('/Volumes/env_dev/leodb/babynames/babynames.csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4916c1ef-20f1-4122-a548-4f59ba7ca022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd44c80-66b3-4386-bf4c-1c5b23ae174f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pyspark\n",
    "from myfunctions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "tableName   = \"babynames\"\n",
    "dbName      = \"leodb\"\n",
    "columnName  = \"First Name\"\n",
    "columnValue = \"CHLOE\"\n",
    "\n",
    "# Because this file is not a Databricks notebook, you\n",
    "# must create a Spark session. Databricks notebooks\n",
    "# create a Spark session for you by default.\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('integrity-tests') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Create fake data for the unit tests to run against.\n",
    "# In general, it is a best practice to not run unit tests\n",
    "# against functions that work with data in production.\n",
    "schema = StructType([ \\\n",
    "  StructField(\"Year\",     IntegerType(), True), \\\n",
    "  StructField(\"First Name\",   StringType(),  True), \\\n",
    "  StructField(\"County\",     StringType(),  True), \\\n",
    "  StructField(\"Sex\",   StringType(),  True), \\\n",
    "  StructField(\"Count\", IntegerType(), True), \\\n",
    "])\n",
    "\n",
    "data = [ (2025, \"Leo\", \"Albany\",   \"M\", 10 ), \\\n",
    "         (2024, \"Alex\", \"Albany\",   \"M\", 8  ) ]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Does the table exist?\n",
    "def test_tableExists():\n",
    "  assert tableExists(tableName, dbName) is True\n",
    "\n",
    "# Does the column exist?\n",
    "def test_columnExists():\n",
    "  assert columnExists(df, columnName) is True\n",
    "\n",
    "# Is there at least one row for the value in the specified column?\n",
    "def test_numRowsInColumnForValue():\n",
    "  assert numRowsInColumnForValue(df, columnName, columnValue) > 0"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6313892620139638,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "L3_DLT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
